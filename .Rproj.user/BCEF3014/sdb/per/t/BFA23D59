{
    "contents" : "---\ntitle: \"Exam\"\nauthor: \"Sebastian Gregersen and Jeppe Korsgaard Kristensen\"\ndate: \"10th june 2016\"\noutput: html_document\n---\n```{r}\ninstall.packages(\"devtools\", repos = 'http://cran.us.r-project.org')\nlibrary(devtools)\ninstall_github(\"greger93/temp2\")\nlibrary(temp2)\n```\n\n√Ündrer n i 2.3 og 2.4 a) og 5.3\n3.3 ikke lavet, printer ikke h i task 5\n\n## Task 1\n\n#### 1\n\nThe data is read from github. The data is grades compared to the amount og sports a person is doing.\n```{r}\nData <-read.table(\"https://raw.githubusercontent.com/haghish/ST516/master/data/sport.txt\")\n```\nThe correlation is found by \n```{r}\nCorr <- cor(Data[,1],Data[,2])\nCorr\n```\nThe correlation states that there is a decent relation between doing sports and getting better grades.\n\n#### 3\n\nPart 3 is made before part 2 since part 3 makes a function which is used to solve part 2.\nThe function is Bootstraptask1 in temp2\n\n#### 2\n\nThe function from task is used with 500 estimations and the data set loaded from github.\n```{r}\nBootstraptask1(500,Data$Sport,Data$Grades)\n```\nThe standard error is the mean of the difference between the 2 vectors. Since this is close to 0 it is a quite good standard error.\nBias is the difference between the estimated correlation and the exact correlation. The value is close to zero and therefore there is not a big difference between the estimated correlation and the exact correlation.\nThe confidence interval is found with a significance level of 95 %. Since the exact correlation is in the interval we can say with 95 % certainty that the estimated correlation is correct.\n\n#### 4\nWe dident want to include additional arguments.\n\n#### 5\n\nThe data is plottet and R's build-in function lm is used to create a linear model of the data set.\n```{r}\nplot(Data[,1],Data[,2], xlab = \"sport\", ylab = \"Grades\")\nabline(lm(Data[,2]~Data[,1]))\n```\n\nIt is clear that from this data set that a higher amount of sport would help you to get better grades.\n\n## Task 2\n\n#### 1\n\nMonte Carlo integration is used to solve the integral between 0 and pi.\n```{r}\nn <- 10000\nestimations <- c()\nfor(i in 1:n) {\n  x <- runif(1)\n  integral <- sin(pi*x)\n  estimations <- c(integral, estimations)\n}\nphit <- sum(estimations)/n\nse <- sd(estimations)/sqrt(n)\nprint(paste(\"Variance:\", var(estimations)))\nprint(paste(\"Standard deviation:\", sd(estimations)))\nprint(paste(\"Standard error:\", sd(estimations)/sqrt(n)))\nconfidence <- qnorm(1-(1-0.95)/2)\nprint(paste(\"Confidence interval:\",mean(estimations) + se*confidence))\nprint(paste(\"Confidence interval:\",mean(estimations) - se*confidence))\nprint(paste(\"Probability of hit:\", phit))\n```\n\nWith a distance of 1 between the parallel lines and a length of the needle equal to 1, there is approximate 67 % chance that a needle hits a parallel line.\n\n#### 2\n\nA function is made to calculate the estimated pi.\nThe function is called Buffontask2 in temp2.\n```{r}\nBuffontask2(10000,1 , 1)\nPhit <- 2*1/(Buffontask2(10000,1 , 1)*1)\nPhit\n```\n\nThe estimated probabiliy of hit from Buffons experiment is close to the estimated probability of hit from the Monte Carlo intergration\n\n#### 3\n\n```{r}\nestpi = c()\nestpi <- append(estpi,Buffontask2(1,1,1))\nestpi <- append(estpi,sapply(seq(10,100,10),function(N) Buffontask2(N,1,1)))\nplot(estpi,xlim = c(1,length(estpi)), ylim = c(min(estpi), max(estpi)))\nabline(a = pi, b = 0, col = 'green')\n```\n\nAccording to the strong law of large numbers it seems like the estimated\npi converges to pi as N increases.\n\n#### 4\n##### a)\n```{r}\ncount = 0\nn <- 100\nfor(i in 1:n){\n  c <- Buffontask2(3408,2.5,3)\n  if(round(c,digits = 6) == round(pi,digits = 6)){\n    count = count + 1\n  }\n}\n100*count/n\n```\n\nIt is therefore very unlikely that by 1 attempt you would estimate pi with 6 digits\n\n##### b)\n\nWith d  = 3 and l = 2.5 the value N = 3408 was a exellent choice for N however the chance that he would get 1808 hits is very unlikely and timeconsuming since from task 4 a) we see that the probability is often lower than 2 percent. We think it is obvious that he knew the fraction found by the chinese matematician, since he chose those values for N, l and d.\n\n## Task 3\n\n#### 1\n\nA function is made to calculate the probability that the chi-square distribution is larger than a given x. The function is called chiprobability in temp2\n\n#### 2\n\nA function is made to calculate chi-square goodness of fit value. The function is called Goodnessoffittask3 in temp2.\n\n#### 3\n\n??\n\n#### 4\n\nThe data is read from github. The second vector is the amount of cards collected for each of the soccer players. The third vector is the probability of getting a certain card.\n```{r}\nnData <-read.table(\"https://raw.githubusercontent.com/haghish/ST516/5e00636e072db393354d468da5d53a7b7bf5a8d8/data/soccer.txt\")\n```\nH0: the cards are uniformly distributed.\nH1: the cards are not uniformly distributed.\nSince it is stated that the cards are equally likely to get it is known that they are uniformly distributed.\n```{r}\ns <- sapply(1:20,function(x) 1/length(nData[,2]))\nT <- Goodnessoffittask3(nData[,2],s)\n1 - pchisq(T, length(nData[,2])-1)\n```\nIt is seen from the result that it is not significant. The cards are\ntherefore not uniformly distributed as it also can be seen on the following graph:\n\n```{r}\nplot(nData[,2], ylim = c(0,max(nData[,2])))\nabline(a = sum(nData[,2])*1/length(nData[,2]), b = 0, col = 'green')\n```\n\n#### 5\n\n```{r}\nchiprobability(T,19,100000)\n1 - pchisq(T, length(nData[,2])-1)\n```\nThere is a difference between the estimated p-value and the analytic p-value since there is a small chance that there are values larger than the T-value. If you created an infinite amount of numbers the 2 values would be equal.\n\n#### 6\n\n$$P(X^2 > x) = 1 - P(X^2 < x) = 1 - ecdf(x)$$\nBecause it is known that you can calculate the p-value from cumulative distribution function it is seen from the equation above that you also can calculate it from the chi square distribution.\nSince there is a large difference between the expected value and the actual value of each cards the Goodness of fit value will be very large and it is therefore very unlikely that the chi square distribution will be higher than that value and they are therefore not uniformly distributed.\n\n#### 7\n\nThere is calculated a Goodness \n```{r}\nT <- Goodnessoffittask3(nData[,2],nData[,3])\nT\nchiprobability(T, 19, 10000)\n```\nTherefore since the qchisq value is higher than 0.05 we can say with a significanslevel of 5 % that Gittes cards fits the expected distribution.\n\n## Task 4\n\n#### 1\n\nA function is made to fit a linear model on a dependent variable and some predictors. The function is called Matrixregression in temp2.\n\n#### 2\n\n3 random normal distributions with 1000 numbers is created. These 3 vectors is inserted in a matrix.\n```{r}\nr1 <- rnorm(1000)\nr2 <- rnorm(1000)\nr3 <- rnorm(1000)\nRnor <- matrix(c(r1, r2, r3), ncol = 3, nrow = 1000)\n```\nA correlation matrix is made from the defined correlations. The correlation between a distribution and itself is equal to 1.\n```{r}\nCorM <- matrix(c(c(1,0.7,0.3),c(0.7,1,0),c(0.3,0,1)), ncol = 3, nrow = 3)\n```\nThe Choleski decomposition is used to obtain the correlation matrix.\n```{r}\nMa <- chol(CorM)\n```\nThe correlated matrix is multiplied with our normal distributions to get the right correlations between the distribtutions.\n```{r}\nVari <- Rnor%*%Ma\n```\nThe 3 variables are defined\n```{r}\nDep <- Vari[,1]\nPre1 <- Vari[,2]\nPre2 <- Vari[,3]\n```\nTo show that the method create the wanted correlation:\n```{r}\ncor(Dep, Pre1)\ncor(Dep, Pre2)\ncor(Pre1, Pre2)\n```\nA linear model of the dependent and each of the predictors is made and it is seen that the slope equals the correlation\n```{r}\nreg1 <- lm(Dep~Pre1)\nplot(Dep, Pre1)\nabline(reg1)\n\nreg2 <- lm(Dep~Pre2)\nplot(Dep, Pre2)\nabline(reg2)\n```\n\nThere is also created a linear model of the two predictors to see that the correlation between them is 0\n```{r}\nreg3 <- lm(Pre1~Pre2)\nplot(Pre1, Pre2)\nabline(reg3)\n```\n\nOur model is used on the variables\n```{r}\nMatrixregression(Dep~(Pre1+Pre2))\n```\nBecause the data set is made correlated and we have a normal distribution with mean 0 the coefficients will be equal to the correlations since they are equally distributed on each side of the mean.\nIt can not be concluded that the correlation equals the coefficents always since correlations are in the inverval [-1,1] and slope value can be all real numbers. Also if you have a distribution that is not normal data will not be equally distributed on each side of the mean.\n\n#### 3\n\nThe data is presented with all the predictors\n```{r}\nData <-read.table(\"https://raw.githubusercontent.com/haghish/ST516/master/data/height.txt\")\nMatrixregression(Data[,4]~(Data[,1]+Data[,2]+Data[,3]+Data[,5]))\n```\nIt is seen from the estimated coefficents that predictor 3 has an influence on 5.23 if you are male or female. The height of the parents has the biggest influence in your height because the coefficents are quite big and they will be multiplied with height of the mother and the father. The social economy status has the least influence since it is a low coefficient and low numbers.\nH0 is that you cant describe the height of a child as a linear model of the 4 predictors.\nH1 is that you can describe the height of a child as a linear model of the 4 predictors.\nIt is seen from the p value's for each of the predictors that the 3 first predictors is significant but predictor 4 which is the social economy status the p-value is high and it is therefore not significant.\nTherefore H0 and H1 are rejected since some of the predictors are significant but not all. A new hypothesis could be that the 3 thirst predictors influence the height of the child.\n```{r}\nMatrixregression(Data[,4]~(Data[,1]+Data[,2]+Data[,3]))\n```\nIt is seen that the result was significant and the hypothesis is therefore accepted.\n\n## Task 5\n\n#### 1\n\nA function is made that estimates the density using two different methods. The function is called estdens in temp2.\n\n#### 2\n\nA function is made that makes a plot of the density using the same methods. The function is called plotdens in temp2\n\n#### 3\n\n##### a)\n\n```{r}\nestdens(faithful[,1])\nestdens(faithful[,1], method = \"kernel\")\n```\n\n##### b)\n\n```{r}\nplotdens(faithful[,1], n = 200)\nplotdens(faithful[,1], n = 200, method = \"kernel\")\n```\n\n##### c)\n\nIs is seen from the 2 plots that the plot with the kernel method is more smooth than the plot with the naive method. The kernel method works best in general but the naive is better for a normal ditributed sample.\n\n## Task 6\n\n#### 1\n\nA function is made to simulate the stationary distributions of a markov chain. The funtion is called Markovchain in tempt2 package.\n\n#### 2\n\nThe transition probability matrix is made based on the probabilities of the Figure 1. Each row is the probabilities of leaving a state and the diagonal is the probability og staying in each state.\n```{r}\nA = matrix(c(0.2,0.7,0,0,0,0.1,\n             0.3,0,0.7,0,0,0,\n             0,0.5,0,0.5,0,0,\n             0,0,0,0.9,0.1,0,\n             0,0,0,0.25,0.5,0.25,\n             0.4,0,0,0,0.4,0.2),\n             nrow = 6, ncol = 6, byrow = TRUE)\nMarkovchain(A,3,10000)\n```\n\nThe funtions returns the probabilities of being in a state while moving 10000 times. It can be seen, that there is a big probability of staying in state 4 which is clear, while it has 90% chance of moving to itself.\n\n## Task 7\n\n#### 1\n\n##### a)\n\n```{r}\nreject <- 0\na <- 2.5\nb <- 5.5\nn <- 10000\nX <- rep(runif(1), n)\nfor(i in 2:n){\n  U1 <- runif(1)\n  sigma <- dbeta(U1,a,b)/dbeta(X[i-1],a,b)\n  U2 <- runif(1)\n  if(U2 < sigma){\n    X[i] <- U1\n  }else{\n    X[i] <- X[i-1]\n    reject <- reject + 1\n  }\n}\nhist(X, probability = TRUE)\ncurve(dbeta(x,a,b), add = TRUE, col = 'red')\n```\n\nTo estimate the beta distribution with parameter a = 2.5 and b = 5.5, we use the Metropolis-Hastings. We make 10000 iterations to obtain the density. The first element of our vector i a uniform number between 0 and 1. Then we generate a new uniform number and finds the relation between the density of the 2 uniform number in the beta distribution with the given parameters. If this relation is larger than another uniform number we accept it, and the new number in the vector will be the first uniform number generated. If we reject the the relation, the next number will be the same as the privous. To compare the founded density, we compare a histogram of the unifrom numbers with the density plot of the beta distribution with the parameters. As the graph shows, the created sample fits the density very well.\n\n##### b)\n\n```{r}\nreject/n\n```\nThe rejectionrate is found by dividing the amount of rejected times in the algorithm by numbers of iterations. It is almost 50% each time, which says that about every other step in the iteration, the previous number is the same as the next one.\n\n##### c)\n\n```{r}\nplot(X, xlim = c(3500,4000), type = 'l')\n```\n\nToo visualize a part of the chain, wee look at 500 numbers from the generated numbers between 3500 and 4000. Those numbers are bounded with the \"l\" type. Whenever we see a horisontal line in the graph, the new number has been rejected from the algorithm, which will happen almost every other time as mentioned before.\n\n##### d)\n\n```{r}\nset.seed(1)\nrbeta <- rbeta(n,a,b)\nplotdens(rbeta, n = 200, method = \"kernel\")\nplot(density(rbeta(n,a,b)))\n```\n\nThe first plot is the one created with our function, plotdens, and the second plot is one created with R's buildin densityplot of the same random generated numbers. It is seen that the two graphs are almost equal.\n\n",
    "created" : 1465479705347.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1076607650",
    "id" : "BFA23D59",
    "lastKnownWriteTime" : 1465504678,
    "path" : "~/Anvendt matematik/ST522 Beregnm√¶ssig statistik/Exam.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 9,
    "source_on_save" : false,
    "type" : "r_markdown"
}